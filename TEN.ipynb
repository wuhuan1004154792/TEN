{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51409c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, alpha, leaky_slope=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.alpha = alpha\n",
    "        self.leaky_slope = leaky_slope\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=self.leaky_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.input = nn.Linear(self.input_size, self.hidden_size[0])\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.hidden_size[0], nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=1)\n",
    "        self.output = nn.Linear(hidden_size[0], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "class TEN:\n",
    "    def __init__(self, n_features, hidden_size=None, q=2, n_dropouts=10, dropout_prob=0.5, batch_size=64,\n",
    "                 learning_rate=0.001, epochs=50, alpha=0.95, sigma=0, f_correct=0):\n",
    "        self.n_features = n_features\n",
    "        self.q = q\n",
    "        if hidden_size is None:\n",
    "            self.hidden_size = [64, 32]\n",
    "        else:\n",
    "            self.hidden_size = hidden_size\n",
    "        self.n_dropouts = n_dropouts\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.f_correct = f_correct\n",
    "        self.S = None\n",
    "        self.new = None\n",
    "        self.model = None\n",
    "        self.last_model = None\n",
    "        self.loss_fn = None\n",
    "        self.f_scores = None\n",
    "\n",
    "    @staticmethod\n",
    "    def bias(x):\n",
    "        if not all(x[:, 0] == 1):\n",
    "            x = torch.cat((torch.ones(x.shape[0], 1), x.float()), dim=1)\n",
    "        return x\n",
    "\n",
    "    def f_test(self, x, y):\n",
    "        slc = SelectKBest(f_classif, k=x.shape[1])\n",
    "        slc.fit(x, y)\n",
    "        return getattr(slc, 'scores_')\n",
    "\n",
    "    def xavier_initialization(self):\n",
    "        if self.last_model is not None:\n",
    "            weight = torch.zeros(self.hidden_size[0], self.model.input.weight.shape[1])\n",
    "            nn.init.xavier_normal_(weight, gain=nn.init.calculate_gain('relu'))\n",
    "            old_s = self.S.copy()\n",
    "            if self.new in old_s:\n",
    "                old_s.remove(self.new)\n",
    "            for i in self.S:\n",
    "                if i != self.new:\n",
    "                    weight[:, self.S.index(i)] = self.last_model.input.weight.data[:, old_s.index(i)]\n",
    "            self.model.input.weight.data = weight\n",
    "            for h in range(len(self.hidden_size) - 1):\n",
    "                self.model.transformer.layers[h].self_attn.in_proj_weight.data = self.last_model.transformer.layers[h].self_attn.in_proj_weight.data\n",
    "                self.model.transformer.layers[h].self_attn.out_proj.weight.data = self.last_model.transformer.layers[h].self_attn.out_proj.weight.data\n",
    "                self.model.transformer.layers[h].linear1.weight.data = self.last_model.transformer.layers[h].linear1.weight.data\n",
    "                self.model.transformer.layers[h].linear2.weight.data = self.last_model.transformer.layers[h].linear2.weight.data\n",
    "            self.model.output.weight.data = self.last_model.output.weight.data\n",
    "    \n",
    "    def feature_scaling(self, x, scale_factor=0.1):\n",
    "        x = torch.tensor(x) \n",
    "        scaling_factors = torch.rand(x.shape[1]) * scale_factor + 1.0\n",
    "        x_scaled = x * scaling_factors\n",
    "        return x_scaled\n",
    "\n",
    "    def train(self, x, y, batch_size=None, augmentation_ratio=0.3, scale_factor=0.3):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        input_size = len(self.S)\n",
    "        output_size = len(torch.unique(y))\n",
    "        self.model = TransformerNet(input_size, output_size, self.hidden_size, self.alpha)\n",
    "        self.xavier_initialization()\n",
    "        x = x[:, self.S]\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        train_set = []\n",
    "        for i in range(x.shape[0]):\n",
    "            train_set.append([x[i, :].view(1, -1), y[i]])\n",
    "            if torch.rand(1).item() < augmentation_ratio:\n",
    "                x_augmented = self.feature_scaling(x[i, :].view(1, -1), scale_factor=scale_factor)\n",
    "                train_set.append([x_augmented, y[i]])\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        for e in range(self.epochs):\n",
    "            for data, label in train_loader:\n",
    "                input_0 = data.view(data.shape[0], -1)\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(input_0.float())  \n",
    "                loss = self.loss_fn(output, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()   \n",
    "        self.last_model = copy.deepcopy(self.model)\n",
    "\n",
    "    def dropout(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        model_dp = copy.deepcopy(self.model)\n",
    "        for h in range(len(self.hidden_size) - 1):\n",
    "            h_size = self.hidden_size[h]\n",
    "            dropout_index = np.random.choice(range(h_size), int(h_size * self.dropout_prob), replace=False)\n",
    "            model_dp.transformer_layer.self_attn.in_proj_weight.data[:, dropout_index] = torch.zeros(\n",
    "                model_dp.transformer_layer.self_attn.in_proj_weight[:, dropout_index].shape)\n",
    "            model_dp.transformer_layer.self_attn.out_proj.weight.data[:, dropout_index] = torch.zeros(\n",
    "                model_dp.transformer_layer.self_attn.out_proj.weight[:, dropout_index].shape)\n",
    "        dropout_index = np.random.choice(range(self.hidden_size[-1]), int(self.hidden_size[-1] * self.dropout_prob),\n",
    "                                         replace=False)\n",
    "        model_dp.output.weight.data[:, dropout_index] = torch.zeros(model_dp.output.weight[:, dropout_index].shape)\n",
    "        return model_dp\n",
    "\n",
    "    def gradient(self, x, y, model, batch_size=None):\n",
    "        model_gr = TransformerNet(x.shape[1], len(torch.unique(y)), self.hidden_size, self.alpha)\n",
    "        temp = torch.zeros(model_gr.input.weight.shape)\n",
    "        temp[:, self.S] = model.input.weight\n",
    "        model_gr.input.weight.data = temp\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.hidden_size[0], nhead=4)\n",
    "        model_gr.transformer = nn.TransformerEncoder(encoder_layers, num_layers=1)\n",
    "        for h in range(len(self.hidden_size) - 1):\n",
    "            model_gr.transformer.layers[h].self_attn.in_proj_weight.data = model.transformer.layers[h].self_attn.in_proj_weight.data + self.sigma * torch.randn(\n",
    "                model.transformer.layers[h].self_attn.in_proj_weight.shape)\n",
    "            model_gr.transformer.layers[h].self_attn.out_proj.weight.data = model.transformer.layers[h].self_attn.out_proj.weight.data\n",
    "            model_gr.transformer.layers[h].linear1.weight.data = model.transformer.layers[h].linear1.weight.data\n",
    "            model_gr.transformer.layers[h].linear2.weight.data = model.transformer.layers[h].linear2.weight.data\n",
    "        model_gr.output.weight.data = model.output.weight\n",
    "        output_gr = model_gr(x.float())\n",
    "        loss_gr = self.loss_fn(output_gr, y)\n",
    "        loss_gr.backward()\n",
    "        input_gradient = model_gr.input.weight.grad\n",
    "        return input_gradient\n",
    "\n",
    "    def average(self, x, y, n_average, batch_size=None):\n",
    "        grad_cache = None\n",
    "        for num in range(n_average):\n",
    "            model = self.dropout(batch_size=batch_size)\n",
    "            input_grad = self.gradient(x, y, model, batch_size=batch_size)\n",
    "            if grad_cache is None:\n",
    "                grad_cache = input_grad\n",
    "            else:\n",
    "                grad_cache += input_grad\n",
    "        return grad_cache / n_average\n",
    "\n",
    "    def find(self, input_gradient):\n",
    "        gradient_norm = input_gradient.norm(p=self.q, dim=0)\n",
    "        gradient_norm = gradient_norm / gradient_norm.norm(p=2)\n",
    "        gradient_norm[1:] = (1 - self.f_correct) * gradient_norm[1:] + self.f_correct * self.f_scores\n",
    "        gradient_norm[self.S] = 0\n",
    "        max_index = torch.argmax(gradient_norm)\n",
    "        return max_index.item()\n",
    "\n",
    "    def select(self, x, y, batch_size=None):\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        self.f_scores = torch.tensor(self.f_test(x, y))\n",
    "        self.f_scores[torch.isnan(self.f_scores)] = 0\n",
    "        self.f_scores = self.f_scores / self.f_scores.norm(p=2)\n",
    "        x = self.bias(x)\n",
    "        self.S = [0]\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        while len(self.S) < self.n_features + 1:\n",
    "            self.train(x, y, batch_size=batch_size)\n",
    "            input_gradient = self.average(x, y, self.n_dropouts, batch_size=batch_size)\n",
    "            self.new = self.find(input_gradient)\n",
    "            self.S.append(self.new)\n",
    "        selection = self.S\n",
    "        selection.remove(0)\n",
    "        selection = [s - 1 for s in selection]\n",
    "        return selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
